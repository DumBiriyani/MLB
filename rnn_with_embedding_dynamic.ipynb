{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# # import tensorflow as tf\n",
    "from tensorflow.python.keras.layers import Input, LSTM, Dense, Masking, Embedding, concatenate\n",
    "from tensorflow.python.keras.models import Model\n",
    "from tensorflow.python.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.utils import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"./processed_data/dynamic_feats.csv\")\n",
    "df.sort_values([\"year\",\"HomeTeam\"],inplace=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Feature List__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "other_league_cols = [\"nba_season_flag\", \"nfl_season_flag\", \"nhl_season_flag\"]\n",
    "\n",
    "common_features = [\"year\",\n",
    "                    \"DayofWeek\",\n",
    "                    \"HomeTeamGameNumber\",\n",
    "                    \"VisitingTeamGameNumber\",\n",
    "                    \"park_age\",\n",
    "                    \"Month\",\n",
    "                    \"DayofMonth\",\n",
    "                    \"DayFlag\",\n",
    "                    \"NonRegular_ParkFlag\",\n",
    "                    \"StadiumCapacity\"]+\\\n",
    "                    [\"home_payroll\", \"home_top_salary\", \"visiting_payroll\", \"visiting_top_salary\"] +\\\n",
    "                    other_league_cols\n",
    "\n",
    "rolling_feats = ['HomeTeamOffense_Homeruns-1',\n",
    "                'HomeTeamOffense_Homeruns-2',\n",
    "                'HomeTeamOffense_Homeruns-3',\n",
    "                'HomeTeamOffense_Homeruns-4',\n",
    "                'HomeTeamOffense_Homeruns-5',\n",
    "                'HomeTeamOffense_Homeruns-6',\n",
    "                'HomeTeamOffense_Homeruns-7',\n",
    "                'HomeTeamOffense_Strickouts-1',\n",
    "                'HomeTeamOffense_Strickouts-2',\n",
    "                'HomeTeamOffense_Strickouts-3',\n",
    "                'HomeTeamOffense_Strickouts-4',\n",
    "                'HomeTeamOffense_Strickouts-5',\n",
    "                'HomeTeamOffense_Strickouts-6',\n",
    "                'HomeTeamOffense_Strickouts-7',\n",
    "                'HomeTeamPitchers_TeamEarnedRuns-1',\n",
    "                'HomeTeamPitchers_TeamEarnedRuns-2',\n",
    "                'HomeTeamPitchers_TeamEarnedRuns-3',\n",
    "                'HomeTeamPitchers_TeamEarnedRuns-4',\n",
    "                'HomeTeamPitchers_TeamEarnedRuns-5',\n",
    "                'HomeTeamPitchers_TeamEarnedRuns-6',\n",
    "                'HomeTeamPitchers_TeamEarnedRuns-7']\n",
    "\n",
    "prev_match_features = ['Homewin_rate', 'Homeday_league_rank']\n",
    "\n",
    "categorical_feat_cols = [\"VisitingTeam\",\n",
    "                        \"VisitingTeamLeague\",\n",
    "                        \"HomeTeam\",\n",
    "                        \"HomeTeamLeague\",\n",
    "                        \"BallParkID\"]\n",
    "\n",
    "continuous_feat_cols = common_features + rolling_feats + prev_match_features"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Constants and Helper Functions__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGET_COL = \"AttendanceRatio\"\n",
    "\n",
    "# 84 games at most in a season, 22 seasons\n",
    "LEN_TS = 84*22\n",
    "NUM_CONT_FEATURES = len(continuous_feat_cols)\n",
    "NUM_CAT_FEATURES = len(categorical_feat_cols)\n",
    "VALIDATION_YEARS = [2022]\n",
    "\n",
    "ranking_map_cols = [\"Date\",\"Team\"]\n",
    "ranking_info_cols = [\"total_wins\",\"day_league_rank\",\"win_rate\"]\n",
    "\n",
    "def pad_with_bfill(arr_list, max_len):\n",
    "    # Pad each array with backfill to the length of the longest array\n",
    "    padded_arr_list = []\n",
    "    for arr in arr_list:\n",
    "        padded_arr = np.pad(arr, pad_width=[(max_len - len(arr), 0),(0,0),(0,0)], mode='edge')\n",
    "        padded_arr_list.append(padded_arr)\n",
    "    return padded_arr_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "val_idx = df[\"year\"].isin(VALIDATION_YEARS)\n",
    "train_idx = ~val_idx\n",
    "normalizers = {}\n",
    "\n",
    "for col in continuous_feat_cols+[TARGET_COL]:\n",
    "    norm_arr_train = df.loc[train_idx, col].values.reshape(-1,1)\n",
    "    norm_arr_val = df.loc[val_idx, col].values.reshape(-1,1)\n",
    "    normalizers[col] = MinMaxScaler().fit(norm_arr_train)\n",
    "    df.loc[train_idx, col] = normalizers[col].transform(norm_arr_train)\n",
    "    df.loc[val_idx, col] = normalizers[col].transform(norm_arr_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# preprocess categorical columns\n",
    "encoders = []\n",
    "\n",
    "for col in categorical_feat_cols:\n",
    "    encoder = LabelEncoder()\n",
    "    data_cat = df[col].fillna('N/A').astype(str)\n",
    "    df[col+\"_encoded\"] = encoder.fit_transform(data_cat)\n",
    "    encoders.append(encoder)\n",
    "\n",
    "categorical_feat_cols = [col+\"_encoded\" for col in categorical_feat_cols]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training\n",
    "\n",
    "### Reshaping data for RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = []\n",
    "y_train = []\n",
    "x_val = []\n",
    "y_val = []\n",
    "x_train_cat = []\n",
    "x_val_cat = []\n",
    "\n",
    "num_teams = df[\"HomeTeam\"].nunique()\n",
    "\n",
    "for team, team_df in df.groupby([\"HomeTeam\"]):\n",
    "    team_train_df = team_df.loc[train_idx,:]\n",
    "    team_val_df = team_df.loc[val_idx,:]\n",
    "    train_games = len(team_train_df)\n",
    "    val_games = len(team_val_df)\n",
    "    train_features = team_train_df.loc[:, continuous_feat_cols\n",
    "                                ].values.reshape(train_games, NUM_CONT_FEATURES, 1)\n",
    "    train_labels = team_train_df.loc[:, TARGET_COL\n",
    "                               ].values.reshape(train_games, 1)\n",
    "    val_features = team_val_df.loc[:, continuous_feat_cols\n",
    "                                ].values.reshape(val_games, NUM_CONT_FEATURES, 1)\n",
    "    val_labels = team_val_df.loc[:, \n",
    "                           TARGET_COL].values.reshape(val_games, 1)\n",
    "    \n",
    "    # Categorical Features\n",
    "    train_cat_features = team_train_df.loc[:, categorical_feat_cols\n",
    "                                ].values.reshape(train_games, NUM_CAT_FEATURES, 1)\n",
    "    val_cat_features = team_val_df.loc[:, categorical_feat_cols\n",
    "                                    ].values.reshape(val_games, NUM_CAT_FEATURES, 1)\n",
    "    \n",
    "    x_train.append(train_features)\n",
    "    y_train.append(train_labels)\n",
    "    x_val.append(val_features)\n",
    "    y_val.append(val_labels)\n",
    "    x_train_cat.append(train_cat_features)\n",
    "    x_val_cat.append(val_cat_features)\n",
    "\n",
    "len_ts_train = df.loc[train_idx,:].groupby(\"HomeTeam\")[\"HomeTeamGameNumber\"].size().max()\n",
    "len_ts_val = df.loc[val_idx,:].groupby(\"HomeTeam\")[\"HomeTeamGameNumber\"].size().max()\n",
    "\n",
    "x_train = pad_sequences(x_train, value=-1, maxlen=len_ts_train).reshape(len_ts_train, num_teams, NUM_CONT_FEATURES, 1)\n",
    "y_train = pad_sequences(y_train, value=-1, maxlen=len_ts_train).reshape(len_ts_train, num_teams, 1)\n",
    "\n",
    "x_val = pad_sequences(x_val, value=-1, maxlen=len_ts_val).reshape(len_ts_val, num_teams, NUM_CONT_FEATURES, 1)\n",
    "y_val = pad_sequences(y_val, value=-1, maxlen=len_ts_val).reshape(len_ts_val, num_teams, 1)\n",
    "\n",
    "x_train_cat = np.array(pad_with_bfill(x_train_cat, len_ts_train))\n",
    "x_val_cat = np.array(pad_with_bfill(x_val_cat, len_ts_val))\n",
    "\n",
    "x_train_cat = [arr.reshape(len_ts_train, num_teams) for arr in np.split(x_train_cat, NUM_CAT_FEATURES, axis=2)]\n",
    "x_val_cat = [arr.reshape(len_ts_val, num_teams) for arr in np.split(x_val_cat, NUM_CAT_FEATURES, axis=2)]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic Model\n",
    "- Implement Batch Normalization! --> Data Leakage otherwise\n",
    "    - Len of TS: 84*20 (number of games per season)*(number of seasons)\n",
    "    - Number of TS: 30 (number of teams)\n",
    "    - Number of Features: 15\n",
    "- Add masking: DONE\n",
    "- Perform pre-padding: DONE\n",
    "- Add normalization: DONE but not batch?!\n",
    "- Target Variable change: Use %age attendance instead of absolute capacity --> DONE but using different logic\n",
    "- Add early stopping: DONE\n",
    "- Add embeddings DONE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants for Training\n",
    "EPOCHS = 50\n",
    "MIN_DELTA = 0.001\n",
    "PATIENCE = 3\n",
    "\n",
    "# add an embedding layer for the categorical inputs\n",
    "categorical_embedding_size = 10\n",
    "\n",
    "# Define the input shape\n",
    "cont_input_shape = x_train.shape[1:3]\n",
    "# cat_input_shape = x_train_cat.shape[1:3]\n",
    "output_shape = 1\n",
    "\n",
    "early_stopper = EarlyStopping(monitor='val_loss',\n",
    "                              mode='min',\n",
    "                              min_delta=MIN_DELTA,\n",
    "                              patience=PATIENCE)\n",
    "\n",
    "## For every categorical variable we have it's own input\n",
    "categorical_inputs = [Input(shape=(num_teams, ),\n",
    "                      name='cat_' + str(i + 1)) for i in range(NUM_CAT_FEATURES)]\n",
    "masked_categorical_inputs = [Masking(input_shape=(num_teams, ))(inp) for inp in categorical_inputs]\n",
    "\n",
    "## Shared embedding layer\n",
    "cat_embedding = Embedding(input_dim=np.max(x_train_cat) + 1,\n",
    "                                output_dim=categorical_embedding_size,\n",
    "                                input_length=len_ts_train)\n",
    "\n",
    "## Repeat this for every categorical column\n",
    "cat_embeddings = [cat_embedding(inp) for inp in masked_categorical_inputs]\n",
    "\n",
    "time_series_input = Input(shape=cont_input_shape)\n",
    "masked_ts_input = Masking(input_shape=cont_input_shape)(time_series_input)\n",
    "\n",
    "## Concatenate the time series input and the embedding outputs\n",
    "x = concatenate([masked_ts_input] + cat_embeddings, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "54/54 [==============================] - 22s 198ms/step - loss: 561836986466304.0000 - val_loss: 0.0133\n",
      "Epoch 2/50\n",
      "54/54 [==============================] - 9s 163ms/step - loss: 259943617789952.0000 - val_loss: 0.0115\n",
      "Epoch 3/50\n",
      "54/54 [==============================] - 9s 164ms/step - loss: 151496867971072.0000 - val_loss: 0.0174\n",
      "Epoch 4/50\n",
      "54/54 [==============================] - 9s 168ms/step - loss: 268600459919360.0000 - val_loss: 1.1913\n",
      "Epoch 5/50\n",
      "54/54 [==============================] - 10s 180ms/step - loss: 146461807345664.0000 - val_loss: 0.0150\n"
     ]
    }
   ],
   "source": [
    "x = LSTM(128,\n",
    "    activation=\"relu\",\n",
    "    return_sequences=True,\n",
    "    dropout=0.2)(x)\n",
    "\n",
    "x = LSTM(64,\n",
    "    activation=\"relu\",\n",
    "    return_sequences=True,\n",
    "    dropout=0.2)(x)\n",
    "\n",
    "x = LSTM(64,\n",
    "    activation=\"relu\",\n",
    "    return_sequences=True,\n",
    "    dropout=0.2)(x)\n",
    "\n",
    "x = Dense(output_shape)(x)\n",
    "\n",
    "model = Model(inputs=[time_series_input]+categorical_inputs, outputs=x)\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "# Train the model with your pandas DataFrame\n",
    "model.fit([x_train]+x_train_cat,\n",
    "          y_train,\n",
    "          epochs=50,\n",
    "          batch_size=32,\n",
    "          validation_data=([x_val]+x_val_cat, y_val),\n",
    "          callbacks=[early_stopper])\n",
    "\n",
    "# Predict the next season (84 games) for all 30 teams (a single time series) using the trained model\n",
    "predictions = model.predict([x_val]+x_val_cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inverse_transform(col, normalizer_dict=normalizers, data=df):\n",
    "    return normalizer_dict[col].inverse_transform(\n",
    "        data[col].values.reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(26786.084402221986, 1.0000015545617773, 845598093.3573964)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import mean_absolute_error, mean_absolute_percentage_error, mean_squared_error\n",
    "\n",
    "y_pred = (predictions[1:,:].reshape(81*30,))*df.loc[val_idx, \"StadiumCapacity\"]\n",
    "y_true = df.loc[val_idx, \"Attendance\"] #y_val[1:,:].reshape(81*30,)\n",
    "mean_absolute_error(y_true, y_pred), mean_absolute_percentage_error(y_true, y_pred), mean_squared_error(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_error(y_test, y_pred, x_test, w1=0.05):\n",
    "    april_mask = x_test[\"Month\"]==0.14285714\n",
    "    mn_twins_mask = x_test[\"HomeTeam\"]==\"MIN\"\n",
    "    april_error = w1*(np.abs(y_test[(april_mask) & (mn_twins_mask)]-y_pred[(april_mask) & (mn_twins_mask)]).sum())\\\n",
    "                    +(1-w1)*(np.abs(y_test[(april_mask) & ~(mn_twins_mask)]-y_pred[(april_mask) & ~(mn_twins_mask)])).sum()\n",
    "    season_error = w1*(np.abs(y_test[mn_twins_mask]-y_pred[mn_twins_mask]).sum())\\\n",
    "                        +(1-w1)*(np.abs(y_test[~mn_twins_mask]-y_pred[~mn_twins_mask]).sum())\n",
    "    return april_error, season_error    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.14285714, 0.28571429, 0.42857143, 0.57142857, 0.71428571,\n",
       "       0.85714286, 1.        ])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[val_idx, \"Month\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.0, 53.95099459695629)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculate_error(y_true, y_pred, df[val_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tensorflow as tf\n",
    "\n",
    "# class FeedBack(tf.keras.Model):\n",
    "#     def __init__(self, units, out_steps):\n",
    "#         super().__init__()\n",
    "#         self.out_steps = out_steps\n",
    "#         self.units = units\n",
    "#         self.lstm_cell = tf.keras.layers.LSTMCell(units)\n",
    "#         # Also wrap the LSTMCell in an RNN to simplify the `warmup` method.\n",
    "#         self.lstm_rnn = tf.keras.layers.RNN(self.lstm_cell, return_state=True)\n",
    "#         self.dense = tf.keras.layers.Dense(output_shape)\n",
    "\n",
    "# def warmup(self, inputs):\n",
    "#   # inputs.shape => (batch, time, features)\n",
    "#   # x.shape => (batch, lstm_units)\n",
    "#   x, *state = self.lstm_rnn(inputs)\n",
    "\n",
    "#   # predictions.shape => (batch, features)\n",
    "#   prediction = self.dense(x)\n",
    "#   return prediction, state\n",
    "\n",
    "# feedback_model = FeedBack(units=32, out_steps=82)\n",
    "\n",
    "# FeedBack.warmup = warmup\n",
    "\n",
    "# def call(self, inputs, training=None):\n",
    "#   # Use a TensorArray to capture dynamically unrolled outputs.\n",
    "#   predictions = []\n",
    "#   # Initialize the LSTM state.\n",
    "#   prediction, state = self.warmup(inputs)\n",
    "\n",
    "#   # Insert the first prediction.\n",
    "#   predictions.append(prediction)\n",
    "\n",
    "#   # Run the rest of the prediction steps.\n",
    "#   for n in range(1, self.out_steps):\n",
    "#     # Use the last prediction as input.\n",
    "#     x = prediction\n",
    "#     # Execute one lstm step.\n",
    "#     x, state = self.lstm_cell(x, states=state,\n",
    "#                               training=training)\n",
    "#     # Convert the lstm output to a prediction.\n",
    "#     prediction = self.dense(x)\n",
    "#     # Add the prediction to the output.\n",
    "#     predictions.append(prediction)\n",
    "\n",
    "#   # predictions.shape => (time, batch, features)\n",
    "#   predictions = tf.stack(predictions)\n",
    "#   # predictions.shape => (batch, time, features)\n",
    "#   predictions = tf.transpose(predictions, [1, 0, 2])\n",
    "#   return predictions\n",
    "\n",
    "# FeedBack.call = call\n",
    "\n",
    "# history = compile_and_fit(feedback_model, multi_window)\n",
    "\n",
    "# IPython.display.clear_output()\n",
    "\n",
    "# multi_val_performance['AR LSTM'] = feedback_model.evaluate(multi_window.val)\n",
    "# multi_performance['AR LSTM'] = feedback_model.evaluate(multi_window.test, verbose=0)\n",
    "# multi_window.plot(feedback_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
